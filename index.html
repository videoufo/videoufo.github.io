<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>VidProM</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="./resources/teasor.png"/>
  	<meta property="og:title" content="VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models." />
  	<meta property="og:description" content="VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 Million unique text-to-Video Prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models." />
    <meta property="twitter:title"         content="VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models." />
    <meta property="twitter:description"   content="VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models." />
    <meta property="twitter:image"         content="./resources/teasor.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models
    </div>

    <div class="venue">
        NeurIPS 2024
    </div>

    <br><br>

    <div class="author">
        <a href="https://wangwenhao0716.github.io/">Wenhao Wang</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ&hl=en">Yi Yang</a><sup>2</sup>
    </div>
    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>University of Technology Sydney</div>
    <div class="affiliation"><sup>2&nbsp;</sup>Zhejiang University</div>

    <br><br>

    <div class="links"><a href="https://arxiv.org/abs/2403.06098">[Paper]</a></div>
    <div class="links"><a href="https://huggingface.co/datasets/WenhaoWang/VidProM">[Data]</a></div>
    <div class="links"><a href="https://github.com/WangWenhao0716/VidProM">[Github]</a></div>
    <div class="links"><a href="https://wisemodel.cn/datasets/WenhaoWang/VidProM">[Wisemodel]</a></div>
      <div class="links"><a href="https://huggingface.co/WenhaoWang/AutoT2VPrompt">[AutoPrompt]</a></div>

    <br><br>
    <div style="text-align: center; width: 80%; margin: 0 auto;">
          <p>ðŸŽ‰ Accepted by <strong>NeurIPS 2024</strong> Datasets and Benchmarks Track. </p>
          <p>âœ¨ <a href="https://web.archive.org/web/20240319061213/https://huggingface.co/datasets"><strong>Top6</strong>/121,084</a> in the Hugging Face Dataset Trending List on Mar. 19th 2024.</p>
          <p>ðŸ”¥ Featured in <strong>Daily Papers</strong> by AK on Mar. 12nd 2024.</p>
          <p>ðŸŒŸ Downloaded <strong>10000+</strong> on WiseModel.</p>
    </div>


    <br><br>

    <img style="width: 80%;" src="./resources/teasor.png" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
        VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models.
    </p>

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 Million unique text-to-Video Prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models.
    </p>

    <br><br>
    <hr>

    <h1>Datapoint</h1>
    <img style="width: 80%;" src="./resources/datapoint.jpg"
         alt="A data point in the proposed VidProM"/>
    <br>
    
    <br><br>
    <hr>

    <h1>Basic information of VidProM and DiffusionDB</h1>
    <img style="width: 80%;" src="./resources/compare_table.jpg"
         alt="Results figure"/>

    <br><br>
    <hr>

    <h1>Differences between prompts in VidProM and DiffusionDB</h1>
    <img style="width: 80%;" src="./resources/compare_visual.png"
         alt="Results figure"/>

    <br><br>
    <hr>

    <h1>WizMap visualization of prompts in VidProM and DiffusionDB</h1>
    <img style="width: 80%;" src="./resources/WizMap_V_D.jpg"
         alt="Results figure"/>
    <div class="links"><a href="https://poloclub.github.io/wizmap/?dataURL=https://huggingface.co/datasets/WenhaoWang/VidProM/resolve/main/data_vidprom_diffusiondb.ndjson&gridURL=https://huggingface.co/datasets/WenhaoWang/VidProM/resolve/main/grid_vidprom_diffusiondb.json%20">[Wizmap]</a></div>
    <br><br>
    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org/abs/2403.06098">
            <img class="layered-paper-big" width="100%" src="./resources/vidprom.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models</h3>
        <p>Wenhao Wang and Yi Yang</p>
        <p>NeurIPS, 2024.</p>
        <pre><code>@article{wang2024vidprom,
  title={VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models},
  author={Wang, Wenhao and Yang, Yi},
  journal={Thirty-eighth Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=pYNl76onJL}
}</code></pre>
    </div>

    <br><br>
    <hr>

   <h1>Contact</h1>
    <p style="width: 80%;">
        If you have any questions, feel free to contact <a href="https://wangwenhao0716.github.io/">Wenhao Wang</a> (wangwenhao0716@gmail.com).
    </p>

      <br><br>
    <hr>

  <h1>Works using VidProM</h1>
    <p style="width: 80%;">
      We are actively collecting your awesome works using our VidProM. Please let us know if you finish one.
    </p>
      
    <p style="width: 80%;">
    1. Ji, Lichuan, et al. "Distinguish Any Fake Videos: Unleashing the Power of Large-scale Data and Motion Features." Arxiv 2024.
    </p>
    <p style="width: 80%;">
    2. Xuan, He, et al. "VIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation." EMNLP 2024.
    </p>
    <p style="width: 80%;">
    3. Liao, Mingxiang, et al. "Evaluation of Text-to-Video Generation Models: A Dynamics Perspective." NeurIPS 2024.
    </p>
    <p style="width: 80%;">
    4. Miao, Yibo, et al. "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models." NeurIPS 2024.
    </p>
    <p style="width: 80%;">
    5. Wu, Xun, et al. "Boosting Text-to-Video Generative Model with MLLMs Feedback." NeurIPS 2024.
    </p>
    <p style="width: 80%;">
    6. Liu, Joseph, et al. "SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers." Arxiv 2024.
    </p>
    <p style="width: 80%;">
    7. Wang, Zeqing, et al. "Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body." Arxiv 2024.
    </p>
 

   <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a> and <a href="https://github.com/elliottwu/webpage-template">Shangzhe Wu</a>.
        The code can be found <a href="https://github.com/VidProM/vidprom.github.io">here</a>.
    </p>

    <br><br>
</div>

</body>

</html>
